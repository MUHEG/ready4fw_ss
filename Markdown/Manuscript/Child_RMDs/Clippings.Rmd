---
title: "Clippings"
output: html_document
date: "2023-04-18"
---
<!-- Functions to implement model algorithms can be associated with a module via . -->

<!-- to provide tools for authoring CHEMs, supplying those CHEMs with data and using CHEMs to implement reproducible modelling analyses. -->

<!-- The ready4pack library depends on two other module authoring libraries. <!-- MOVED CONTENT -->

<!-- Each CHEM module authored with the ready4 framework will include both a data structure (specifying the required properties of data that can validly be supplied to a module) and a set of algorithms (specifying the operations that can be performed on data contained in a module instance). -->

<!-- development version -->

<!-- Talk about DARTH (https://doi.org/10.1007/s40273-019-00837-x )/ BCEA -->


#################################



There is a strong case for making health economists more accountable for the appropriate use and social acceptability of their models. Potential users of models should be able to assess their adequacy for a particular purpose [@thompson2019escape].  This goal is difficult to achieve in the current context of the poor reproducibility [@Jalali2021; @McManus2019; @Bermejo2017] and frequently insufficient validation [@Ghabri2019] of health economic models. Many health economic models are released with undeclared errors [@Radeva2020]. The value judgments that strongly shape health economic analyses are rarely made explicit, omissions that may lead to socially unacceptable policy recommendations [@duckett2022journey]. A modelling team's value judgments about what questions to address, the most important features of a system to represent and the weighting of different types of evidence may be poorly aligned with those of the people impacted by decisions informed by model analyses [@thompson2022escape]. As health economic models adopt more sophisticated techniques, the need for accountability grows. More complex models may be more prone to propagation errors [@Saltelli2019] and models designed to address multiple questions should be expected to meet more onerous verification and validation obligations [@Eddy2012; @Feenstra2022]. The nature and extent of individual model authorship contributions may be less clear in models implemented over longer time-frames with a large and changing group of collaborators [@thompson2022escape].

, supplying context specific data as replaceable data-packs and distributing source code under licenses that allow derivative works) that facilitate model transferability.

Health economic models should be updated and refined as new evidence emerges and decision contexts change [@Jenkins2021], but this occurs infrequently [@Sampson2017]. Funding for health economic modelling projects rarely extend to provision of medium term support for model updates and improvements. The career trajectories of health economists can also mitigate against adequate maintenance of a model, it being relatively common for model authors to have moved on from the team that owns the model and / or from working on the health condition for which the model was developed.



Our interest in modular and open source approaches developed when we began seeking am appropriate framework for undertaking and validly synthesising diverse types of economic research in mental health. Mental disorders impose high health, social and economic burdens worldwide [@RN8;@GBD2019]. Much of this burden is potentially avertable [@RN25], but poorly financed and organised mental health systems are ill-equipped for this challenge [@RN22;@RN23]. A substantial economic literature already exists to assess the affordability and value for money of mental health interventions [@RN34]. This economic evaluation work is an essential prerequisite for improving allocative efficiency in mental health, but could be of greater value to systems planners if integrated with a broader program of economic research. 

We are developing ready4 (https://www.ready4-dev.com), a modular OSHEM (MOSHEM) on the mental health of young people aged 12 to 25. We are initially applying ready4 to four of the twelve domains of health economics identified by Wagstaff and Culyer [@wagstaff2012four]:
<!-- There is therefore an opportunity for health economists to undertake mental health research of significant potential public benefit. -->

- *health and its value* (our projects: utility mapping models); 

- *determinants of health and ill-health* (our projects: models for creating synthetic household populations with key risk and protective factors for mental disorders);

- *demand for health and health care* (our projects: spatial epidemiology and help-seeking choice models); and

- *supply of health services* (our projects: a model of primary mental health care services). 

Once these projects are completed, our aim is to flexibly combine these models to answer questions in two additional Wagstaff and Culyer domains:

- *efficiency and equity* (our goal: assess the distributional impacts and identify the optimal targeting of care provision); and

- *economic evaluation* (our goal: assess the cost-utility of competing policy options for improving the mental health of young people).

Standardisation is an essential enabler of automation, collaboration, interoperability and transferability in modular and open source approaches. An early requirement to implement ready4 is therefore to identify an appropriate set of standards. However, we are not aware of any consolidated source of guidance about recommended standards for MOSHEMs. Once identified, the standards specified for ready4 will need to be implemented by software, some of which will require custom development. 

<!-- When planning how to undertake and synthesise this program of work, we became concerned with issues relating to accountability, reusability and updatability. If mistakes are common in individual health economic modelling projects [@Radeva2020], then combining multiple projects may be particularly error prone. The risk of errors may also depend on whether a model is too simple or too complex [@Saltelli2019], so the level of granularity appropriate for one model application may be ill-suited to another. Undertaking a multiannual program of interdependent work raises the risk that models developed in the first phase of a project may need to be updated by the time the project is approaching conclusion. -->

<!-- These concerns led us to seek the MOSHEM approach that provides the transparency to make it easier for errors to be uncovered, the adaptability to fix these errors and the flexibility to facilitate interchangeability between representations of different levels of granularity.  -->


### Standards for an accountable MOSHEM
Guidance on transparency in health economic modelling published over ten years ago [@Eddy2012] made recommendations on documenting models but notably did not include recommendations on sharing model code and data. However, more recent and multidisciplinary healthcare modelling guidance [@Erdemir2020] recommends using existing digital repository services to make these types of digital model artefacts publicly available. Some repositories such as GitHub [@github2007] provide tools for disseminating work in progress code and providing highly transparent records of the complete development history and individual authorship contributions of a software project. Other repositories such as Zenodo [@Zenodo2013] and Dataverse [@Dataverse2007] provide persistent storage solutions that generate a Digital Object Identifier (DOI) for each code and data collection. 

Model code and data should be clearly documented, potentially with different versions for technical and non-technical users [@Eddy2012]. Consistent use of meaningful naming conventions when authoring code is recommended [@Wilson_2017; @Alarid2019]. Code can be made easier to follow by using the practices of abstraction [@8717448], where only simple, high level commands are routinely exposed to reviewers, and polymorphism [@7181447], where the same command (e.g. "simulate") can be reused to implement different algorithms of the same type. Programs to implement model analyses can be made comprehensible to even non-technical users through the use of literate programming techniques and tools like RMarkdown [@xie2018r] that integrate computer code with plain English descriptions. 

An essential component of quality assuring health economic models is verification - ensuring that calculations are correct and consistent with model specifications [@techver2019]. One useful concept for informing model users about the extensiveness of verification checks is code coverage [@ERICWONG2010188] - the proportion of model code that has been explicitly tested. <!-- In R, the testthat [@testthat2011] and covr [@covr2020] tools can be used in conjunction with GitHub to define tests and report coverage metrics. -->Transcription errors - mistakes introduced when transferring data between sources, models and reports - are very common in health economic models [@Radeva2020]. The risk of these errors might be lower if there was full transparency across all steps in a study workflow. Scientific computing tools now make it relatively straightforward to author programs that reproducibly execute all steps in data ingest, processing and reporting [@Wilson_2017].

Code and data should be distributed with tools that make it easy for potential users to appropriately cite each model artefact. 

### Standards for a reusable MOSHEM
```{r echo = F}
gpl_pc_dbl <- get_license_share("GPL")
```
To make model code and data widely re-usable by others, it is important to provide users with appropriate and explicit permissions. In the context of open source models, there are two broad categories of licensing options. Some guidance strongly recommends the use of permissive licensing [@Wilson_2017] that provides users with great flexibility as to the purposes (including commercial) for which the content could be re-used. An alternative approach is to use copyleft licenses [@copyleft2022] that can require content users to distribute any derivative works they create under similar open source arrangements. For code, it may be appropriate to adopt the prevailing open source licensing practice within the programming language being used. For data, it may not be sufficient to simply choose between a permissive license like the Public Domain Dedication (CC0) [@cc02022] or a copyleft option such as the Attribution-Share Alike (CC-BY-SA) [@bysa2022]. In addition to ensuring that data is ethically appropriate for disseminate in open access repositories, responsible custodianship of some de-identified or aggregated data may involve using or adapting template terms of use [@sampleterms2022] which have a number of ethical clauses (for example, prohibiting efforts to re-identify research participants). 

Storing model code and data in separate files and locations (as opposed to hard coding - embedding data into source code) can make it easier to apply models to different decision contexts and to selectively restrict access to data that are confidential, while disseminating all other model artefacts. Clear distinctions should be made between model modules (code that defines abstract data structures and the algorithms that can be applied to data described by these structures), model datasets (digital information such as parameter values, unit records, etc) and model analyses (code that links model datasets to model modules and specifies the algorithms to apply to data associated with each module).

The software development practice of encapsulation [@8717448] can be used to help ensure that model modules continue to work as intended when they are combined [@ready4oop2022]. In some cases, combining modules may mean new versions of modules have to be created to better account for interaction effects. The concept of inheritance [@8717448] can be used to write code that efficiently achieves this objective as well as to facilitate selective editing of modules when transferring models to different decision contexts [@ready4oop2022]. Writing algorithms as collections of functions (short, self-contained and reusable software routines that each perform a discrete task) is recommended as good practice for scientific computing [@Wilson_2017]. Functions to implement model algorithms can be associated with data structures (also known as a class) via a special type of function called a method. Model modules of a similar type or purpose can be efficiently distributed and documented by bundling them as code libraries. It is good practice to make available test or toy data to demonstrate the use of model algorithms [@Wilson_2017].

Statistical models are a common output of health economic evaluations, but they are often not reported in a format that enables others to confidently and reliably re-use them for out of sample prediction [@Kearns2013]. Open source approaches can help address this by disseminating code artefacts that enable easy and appropriate use of a statistical model to make predictions with new data. However, great care must be exercised when publicly releasing model artefacts derived from data on human subjects as they may by default embed a copy of the source dataset. Sensitive dataset copies must therefore be replaced (for example, with synthetic data) and the amended artefact's predictive performance then retested before any public release. Another way to make MOSHEMs easier to use is to develop simple user-interfaces for non-technical users. 

### Standards for an updatable MOSHEM
To avoid MOSHEMs going stale - losing validity and usefulness with time - they should be routinely updated.  Each update of code and data should be uniquely identifiable and retrievable, a goal that can be facilitated by use of version control tools [@Erdemir2020] such as git [@git20XX]. 

Potential users of model code and data should be able to easily identify the appropriateness of a version to their needs. Using semantic versioning [@semver20xx] conventions can signal the potential importance of an update to users of model code and data. Also informative is to clearly label the type of code versions hosted in a repository as either:

- "development" - typically the most comprehensive and up to date source code, but potentially not yet sufficiently well documented and tested for potential users to apply to purposes other than testing;

- "production" - code that has been released with a view to it being applied to its stated purposes and therefore typically required to meet defined documentation and testing standards; or

- "archive" - permanent copies of code at key milestones in its development that can aid study reproductions by making available the version of code used in a specific analysis.

Continuous integration [@CI2017] tools can help verify that each code update continues to pass a standardised battery of quality tests. Finally, using deprecation conventions that take an informative and staged approach to retiring old code and data reduces the risk of model revisions having unintended consequences on third party users.  



We developed a toolkit to help us develop and use MOSHEM modules, datasets and analyses that meet the standards listed in Table \@ref(tab:timelygls). The toolkit is comprised of online repositories and software.

## Repositories <!-- Existing ref -->
We created a GitHub organisation (a collection of code repositories) where all our development code is stored and version controlled [@ready4gh2022]. We configured the repositories in our GitHub organisation to use GitHub actions to support continuous integration. Some of the continuous integration checks we have defined assess each library's compliance with policies specified by the Comprehensive R Archive Network (CRAN) [@CRAN2022], to which we plan submitting future production releases. To track our code coverage, we linked our GitHub organisation to an account we established at codecov [@codecov_2022]. 

We enabled GitHub Pages in each repository we used for code library development to facilitate the creation and hosting of library documentation websites. We also developed a consolidated and versioned project documentation website [@rfwn2022] using the Hugo framework [@hugo_2023] and Docsy theme [@docsy_2023]. To host that website we established an account with Netlify [@netlify_2023] and linked that service to our GitHub organisation so that the website would automatically update whenever the source code in its GitHub repository was edited.

We also created a Zenodo community [@ready4zen2022] - a collection of permanent, uniquely identified repositories. We then linked our Zenodo community and GitHub organisation so that every time we specify a version of code in one of our GitHub repositories as a "release", a copy of that code is automatically created on Zenodo with a DOI. Finally, to manage model datasets, we created a dedicated collection within the Harvard Dataverse installation [@ready4dv2022].

## Software <!-- Novel ref -->
We created six development version R libraries to help us author model modules, supply those modules with data and implement reproducible modelling analyses. The six libraries, their primary focus, the standards they support and the third-party packages they depend on are summarised in Table \@ref(tab:cpkgs). 

A library called ready4 [@ready42022] defines a template module (using R's S4 class system) from which all model module data structures will inherit features and a novel syntax for attaching algorithms to those data structures. The ready4 library also contains tools for retrieving web based information on model modules, datasets and analysis programs and for partially automating updates to the project documentation website.

Three R libraries are designed to standardise and partially automate workflows for authoring new model modules. The ready4pack library [@ready4pack2022] is designed to integrate with our GitHub organisation and provides tools for authoring module libraries that are:

- documented (with a website, a manual itemising selected contents and a manual itemising all contents);

- licensed (using GNU GPL-3 [@GNUGPL2022] by default);

- easily citable (citation information can be retrieved within an R session or from hosting repositories); and 

- quality assured (each update triggers continuous integration workflows, including any unit tests created by module library authors). 

The ready4pack library depends on two other module authoring libraries. Methods from the ready4fun library [@ready4fun2022] are used to verify that functions for implementing module algorithms are written in a consistent house style. That standardised format is then used by ready4fun methods to automatically generate basic documentation for each function. Methods from the ready4class [@ready4class2022] library are used to streamline and standardise the authoring of module data structures and the linking of methods to these data structures. Like ready4fun, the ready4class library uses standardised code implementation to automatically generate basic documentation for each module data structure.

The ready4use library  [@ready4use2022] contains modules for ingesting model datasets from online repositories (hosted on a Dataverse installation or on GitHub), labelling model datasets and sharing model datasets via online repositories. The ready4show library [@ready4show2022] contains tools to help author analyses programs that are either self-documenting or which trigger the creation of a scientific summary.

When used in conjunction with toolkit repositories, the six R libraries provide support for implementing 17 out of 20 framework standards (Table \@ref(tab:timelygls)). Standards relating to safe dissemination of statistical models (R8), user-interface development (R9) and deprecation conventions (U4) are better met through using existing third party R libraries. Preparing statistical models for dissemination can be accomplished with standard R data management tools like the dplyr [@dplyr2022] and purrr [@dplyr2022] libraries. User-interfaces are typically developed with the shiny [@shiny2022] library, for which a tutorial aimed at health economists is available [@SmithR2020]. The library lifecycle [@lifecycle2021] provides tools for R developers to consistently deprecate their code.



We applied the framework to develop an initial set of ready4 modules, supply those modules with data and implement modelling analyses. These outputs were created as part of a previously described study [@Hamilton2021.07.07.21260129] to develop utility mapping models appropriate for use in samples of young people presenting to primary mental health services. The ready4 framework's modelling toolkit created the following artefacts:

 - development version module libraries for describing and validating youth mental health human record datasets [@hamilton_matthew_2022_6084467], scoring health utility [@hamilton_matthew_2022_6084824], specifying utility mapping models [@hamilton_matthew_2022_6116701] and implementing reproducible utility mapping studies [@gao_caroline_2022_6130155];
 
 - a development version library of functions for finding and using utility mapping models developed with these tools [@matthew_p_hamilton_2021_5646669];
 
 - data collections of synthetic populations for testing model modules [@DVN/HJXYKQ_2021] and study input and results data [@DVN/DKDIB0_2021]; 
 
 - programs for replicating all steps from data ingest to manuscript reporting [@hamilton_matthew_2022_6129906], applying utility mapping models to new data [@hamilton_matthew_2022_6416330] and generating a synthetic representation of the study dataset [@hamilton_matthew_p_2022_6321821];
 
 - subroutines for creating a catalogue of utility mapping models [@hamilton_matthew_2022_6116385] and generating a draft scientific manuscript [@matthew_p_hamilton_2022_5976988] for studies implemented with these modules.
 
We created a checklist (Table \@ref(tab:checktb)) that we used to assess the extent to which study outputs met framework standards. We assessed these outputs as wholly or mostly meeting 18 out of 20 standards. The two standards where the study outputs currently fall short are in reporting code coverage and including a user-interface. Both these items are scheduled to be addressed when we release production versions of the code libraries.


In this article we described a framework that we developed to help us implement ready4 - a MOSHEM in youth mental health. We outlined framework standards for an accountable, reusable and updatable MOSHEM and described the modelling toolkit we created for applying those standards to the development and use of the ready4 MOSHEM. We also provided an overview of an initial set of MOSHEM modules developed with the framework to implement a utility mapping study. We reviewed the modules, datasets and analyses generated by that study against framework standards. The work we have described has potential implications for the development of the ready4 MOSHEM and for health economic modelling in mental health. A number of issues have more general relevance to health economic modellers and funders of health economic research.

## Implications for implementing ready4
The most direct implication of the development of the ready4 framework is that it makes it feasible for us to implement a MOSHEM in youth mental health. The standards specified by the framework have enabled us to partially automate workflows for developing and applying ready4 through use of the framework's modelling toolkit. We have demonstrated the practical utility of the modelling toolkit by applying it to authoring, documenting and disseminating ready4 module libraries  [@hamilton_matthew_2022_6084467; @hamilton_matthew_2022_6084824; @hamilton_matthew_2022_6116701; @gao_caroline_2022_6130155], datasets [@DVN/HJXYKQ_2021; @DVN/DKDIB0_2021]; analyses [@hamilton_matthew_2022_6129906; @hamilton_matthew_2022_6416330; @hamilton_matthew_p_2022_6321821], reporting templates [@hamilton_matthew_2022_6116385; @matthew_p_hamilton_2022_5976988] and prediction tools  @matthew_p_hamilton_2021_5646669] used in a utility mapping study [@Hamilton2021.07.07.21260129]. The standardised and partially automated workflows used in creating and sharing these artefacts has the potential to generate significant efficiencies as we apply the ready4 framework to undertaking new economic studies. 

We have also been able to demonstrate the interoperability of the initial ready4 modules developed with the modelling toolkit. The program used to implement the utility mapping analysis [@hamilton_matthew_2022_6129906] combines modules from four module libraries ([@hamilton_matthew_2022_6084467; @hamilton_matthew_2022_6084824; @hamilton_matthew_2022_6116701; @gao_caroline_2022_6130155]) and two framework libraries [@ready4use2022; @ready4show2022]. Example literate programs published on the ready4 documentation website [@rfwn2022] use toy data [@DVN/HJXYKQ_2021] to illustrate the potential for ready4 modules to facilitate study replication and transferability. As demonstrated by the checklist we developed (Table \@ref(tab:checktb)), our framework's standards also provide a mechanism for to assess the extent to which the ready4 MOSHEM meets explicit objectives.  

However, having features that facilitate accountability, reuse and updating is not the same as being accountable, reused and updated. If diverse groups of stakeholders do not review model components, suggest improvements and develop alternatives, then little progress is made towards enhancing model legitimacy. Similarly, making code and data publicly available does not guarantee that others will know of the existence of these tools, trust their validity and find them easy to use. Without reuse, errors in model artefacts are more likely to remain undetected. Even when errors are detected, they still need to be fixed, but maintaining code and data requires ongoing resourcing through a combination of centralised infrastructure and an active open source community.

To progress from a technical capability to behavioural outcomes, both our framework and MOSHEM need further work. Currently all the framework and model module libraries we have developed are available only as "development" releases. An early priority for us is to undertake the additional development, testing and documenting of these libraries so that we can submit production versions of each library to CRAN [@CRAN2022]. Making an R library available on CRAN is normally a prerequisite for a high level of use. 

The transferability claims we make for our existing modules are to date supported only by example programs using toy data. Our future work aims to address this with real world studies that apply modules to different concepts and contexts. Our current work program also aims to create new ready4 modules for modelling help-seeking choice, spatial epidemiology, household populations and primary mental health services that we hope will provide others with more reasons to use ready4 and contribute to its development. To facilitate code contributions by third parties, our libraries for authoring modules [@ready4pack2022: @ready4fun2022; @ready4use2022] require some additional development to make them easier to use by third parties without knowledge of the naming and directory structure conventions we use in authoring code.


## Implications for economic modelling in mental health
Open source approaches have been recommended to help develop the mental health modelling field [@RN73] but only one mental health related model (in Alcohol Use Disorder [@Basu2018]) is currently indexed in the Open Source Models Clearinghouse [@OSMC_20xx;@Emerson2019]. We are aware of just one other open source mental health model - a reference model in Major Depressive Disorder - that is currently in development [@IVIMDD2022]. Of the known barriers to adoption of open source models by health economists (including issues like intellectual property and confidentiality [@Pouwels2022; @Wu2019]), our experience suggests that the biggest challenges may be the enormous effort required to first prepare model code and data for public release in formats that facilitate appropriate reuse by third parties and to then maintain and continually improve potentially large numbers of digital artefacts. 

Automated tools such as those we developed in our modelling toolkit can help reduce the burden associated with some of these tasks. However, we think the current low rates of adoption by health economists of open source approaches [@Emerson2019; @Michalczyk2018; @Feenstra2022] (which in turn facilitate the collaboration that make modular models more attractive) will only change slowly unless there is significant and strategic investments made by research funders. Currently, incentive structures for health economists do not promote the dedication of large quantities of time to enable peers to reuse their work.<!-- MH Research priorities (WP3 - Research, WP6 - Economic) https://doi.org/10.1016/S2215-0366(15)00332-6 -->

Reducing waste in research is a responsibility of research funders [@chalmers2014increase] and the poor reproducibility [@Jalali2021; @McManus2019; @Bermejo2017], limited reusability [@Emerson2019; @Michalczyk2018; @Feenstra2022] and uncertain validity [@Ghabri2019; @Radeva2020] of health economic models is wasteful. Approximately 4,000 mental health focused economic evaluation reports were produced between 2000 to 2019 [@RN34]. The intellectual asset represented by this literature could be enhanced if many of the models described in these reports could be brought and kept up to date and made available in formats that maximised transferability to diverse decision contexts. We believe that modular and open source approaches would be well suited to accomplishing this goal and that the framework we have developed could act as an early prototype for solving some of the technical challenges of this task. Ideally such a program of research would be resourced to be sustained over the medium to long term and to engage a diverse network of investigators, contributors and advisers from high, middle and low income countries.

In addition to extracting more value from the existing health economic knowledge base in mental health, there is an opportunity for research funders to shape how future health economic models in mental health are undertaken. Mental health topics accounted for 268 of the 2829 (10%) peer reviewed economic evaluations undertaken during 26 month period in 2012-2014 identified by a review [@pitt2016economic]. The ongoing annual output of economic research in mental health that focuses on the other 11 domains identified by Wagstaff and Culyer [@wagstaff2012four] is probably also substantial. Funders should provide support for the projects and infrastructure to promote greater collaboration, interoperability, transferability and maintenance of future mental health modelling projects.

Developing networks of modellers working on common health conditions has been recommended as a strategy for improving model validity [@Sampson2019] and some of us are part of a nascent initiate of this type in mental health [@whiteford_bagheri_2022]. Collaboration between teams of health economists can make some complex modelling projects more feasible [@Arnold2010] and the significant deficits in our understanding of the systems in which mental disorders emerge and are treated [@Fried2020] suggest that there are a number of candidate topics in mental health that might benefit from pooling of efforts. The weak theoretical underpinnings for understanding complex mental health systems [@RN2111] may be a place to start. It remains unclear why increased investments in mental health care have yet to discernibly reduce the prevalence and burden of mental disorders[@RN26].  The  literature, and evidence base, regarding how the requirements, characteristics and performance of mental health services are shaped by spatiotemporal context needs to be further developed [@RN42]. There is also a need for better evidence to identify the social determinants of mental disorders most amenable to preventative interventions, and for which population sub-groups such interventions would be most effective [@RN43]. 

Ideally health economists would explore these complex topics in partnership with modellers from other disciplines (in particular epidemiology and health services research) and a wide range of stakeholders such as other researchers, policymakers, service planners and community members. Modular and open source approaches would facilitate such investigations by breaking down ambitious and long term goals into manageable time-bound discrete projects, each progressed by different teams. To facilitate such an approach, a common framework of standards and tools would be needed. To be suitable for such a task our framework would need additional development, with its overall architecture reviewed for scalability and suitability and to provide better integration with and use of other open source languages (particularly python) and repositories. Whatever MOSHEM infrastructure is developed, its resilience would depend on a community of open source contributors sufficiently large and active to ensure that all core modules are maintained even after their original authors cease their involvement.

## General issues for health economists and health research funders
Some of the issues we have discussed in the context of the development of our model or health economic modelling in mental health are potentially relevant to health economists and funders of health economic research more generally. Proactive measures by funders to encourage more accountable, reusable and updatable health economic models is not a need confined to mental health. For example, funders have been encouraged to support methodological innovation to improve model transferability [@craig2018taking]. However, funders also need credible proposals to support and this is an area for health economists interested in MOSHEMs to prioritize. Health economists could use existing and new special interest groups to identify opportunities and enablers of more collaborative approaches to model development, potentially as the basis for future funding proposals. 

Adopting MOSHEMs will expand the type of skillset typically engaged in health economic modelling projects, with a much greater role for data-scientists, software engineers and online community builders. The requirement for these roles should be incorporated into project proposals. Not all efforts by health economists to promote MOSHEMs need to depend on the decisions of research funders. Releasing selected subsets of unmaintained model artefacts in open source repositories is still better than not providing access to any code and data and can typically be accomplished within existing project budgets. Developing knowledge and skills of MOSHEMS can be advanced by making small contributions (e.g. improvements to documentation, code contributions) to open source projects. Our project website [@rfwn2022] includes details of multiple ways to contribute to ready4.

<!-- Estimating the potential uptake of these tools is difficult.  -->
<!-- Open source is more than open access (limited discussion). -->
 <!-- A coding framework for OSHEMs developed in the language R includes standardised approaches to directory structure and naming conventions [@Alarid2019]. -->
<!-- http://hdl.handle.net/1893/27205--><!-- CONTEXT: Funders should also support methodological work on context, including the development of reporting templates to facilitate accurate, comprehensive and systematic reporting of context, approaches to evidence synthesis that can capture contextual variation adequately and modelling methods that can leverage the accumulating evidence on contextual variation to predict the reach, implementation and effectiveness of promising interventions in new contexts." --



<!-- (described in more detail on the project documentation website [@rfwn2022])  -->
<!-- Sustained funding -->


 <!-- - One shot at big changes -->
 <!-- - Our analysis - do it in stages, be transparent, continually improve, get help, be flexible (modelling toolkit / monolithic model), have a framework -->
<!-- Diversify skills - move to tools means software engineering, -->
<!-- More collaboration -->
<!-- Role for SIGs -->
<!-- Make grant proposals that give funders the opportunity to fund this work. -->
<!-- Share some code and data at small scale. -->


<!-- Standardized and transferable approaches to health economic modelling have been recommended. [doi:10.1017/S0266462317000666] -->
<!-- As modelling methods have become more sophisticated, some have argued that more complex models are required to adequately explore certain types of public health topics [@SQUIRES2016588].  -->
<!-- Modellers should be faithful to reaity and values of users DOI: 10.1002/asi.23697 -->
<!-- transferability - [https://doi.org/10.1017/S026646232200321X].  -->
<!-- Output by health economists [@pitt2016economic; @wagstaff2012four; @munoz2020health] -->
<!-- - Can develop guidelines and funding proposals, make small amounts of artefacts online -->

<!-- Reducing waste in research is responsibility of research funder: https://doi.org/10.1016/S0140-6736(13)62229-1 -->

<!-- greater use of these types of models may require adaptation on the part of funders, modellers and decision-makers. T -->

<!-- Link to Wagstaff - Demand forecasting, valuation of health and healthcare, economic evaluation, utility mapping?? -->

<!-- # Motivation -->

<!-- ## Why develop OSHEMs in mental health -->


<!-- Major mental health reform programs, <!-- such as those currently being implemented in Victoria, Australia [@VicRC2021],  -->
<!-- can involve the identification, prioritisation, sequencing, targeting and monitoring of multiple interdependent initiatives.   -->



<!-- The development, validation and updating of more complex mental health economic models implemented over longer time frames may be too onerous a burden for a single modelling team.  -->


<!-- Decision makers are frequently left to rely on economic analyses that become less valid and relevant each year. A model can grow stale when changes in the real world, whether sudden (e.g. new pandemics) or gradual (e.g. demographic), is not represented by corresponding updates to relevant model features. Even when the system being modelled remains relatively stable, new research findings may potentially invalidate aspects of a model's original conceptualisation and implementation.  -->

<!-- that rarely explores complex dynamic systems  -->
<!-- [PECUNIA] -->

<!-- Computational modelling could play an important role in developing policies to improve population mental health but this may require significant changes in the way mental health modelling projects are funded, conceptualised and implemented.  -->
 <!-- but, as with health economics more generally, OSHEMs remain rare. Greater use of open source approaches could help improve the scope, validity and usefulness of mental health economic models. -->
 
<!-- Repositories such as Zenodo [@Zenodo2013] and Dataverse [@Dataverse2007] provide persistent storage solutions that generate a Digital Object Identifier (DOI) for each unique item. These repositories are a preferable solution for sharing citable code and data than transitory repositories such as corporate websites or GitHub where items can be deleted or relocated at any time [@sseditors2022]. Zenodo includes tools that automate integration with GitHub, which makes it easy for developers to maintain parallel code repositories - one for disseminating the most up to date development code and the other for archiving citable code releases. --> <!-- Developers storing data in a Dataverse installation have access to multiple meta-data fields to document both a data collection and its individual constituent files.  --><!-- In R, code manuals and websites can be created with the aid of tools such as devtools [@devtools2021], sinew [@sinew2022], roxygen2 [@roxygen2021] and pkgdown [@pkgdown2022].  -->
 
<!-- and the use of version control systems. <!-- such as Git [@git20XX].  -->  
 
<!-- Applying a previously published algorithm [@kross2016] to analyse the most comprehensive archive of released R packages [@CRAN2022] finds that `r gpl_pc_dbl`% are distributed under various forms of General Public License (GPL) [@GNUGPL2022], a copyleft license. -->
<!-- In R, including a CITATION file in the `inst` directory of a package will enable users of that package to retrieve citation information by running a command of the format `citation("Package Name")` in the R console [@Salmon2021]. More generally, including a CITATION.cff file at the top level of your code repository will enable GitHub and Zenodo repositories hosting that item to include the relevant information in their citation tools [@Druskat_Citation_File_Format_2021]. Datasets hosted on Dataverse installations have metadata fields that, once completed by authors, generate citation files for dataset viewers. -->

<!-- A key challenge to generalising health economic models is that they are typically developed to inform a decision problem with a highly specific jurisdictional context. However, a number of choices about how these models are implemented can significantly increase the re-usability of model code in other contexts.  -->
<!-- For example, when generalising a model developed for the Australian context to a UK context, one could create a class that initially inherits all of the methods defined for the Australian model and then write new or replacement methods as needed for the UK model.  -->

<!-- A number of tools and approaches can make the process of implementing and curating changes to model code and data more coherent and efficient.   <!-- Repositories such as Zenodo [@Zenodo2013] and Dataverse [@Dataverse2007] provide persistent access to all published versions of a dataset, each uniquely identifiable. For code, use of version control tools like Git [@git20XX] can ensure that the entire development history of a project is organised so that each version is distinguishable and retrievable by developers. The online platform GitHub [@github2007] can make this version history accessible to anyone. -->

<!-- For R code, the usethis [@usethis2021] package can be used to partially automate version number increments using the convention Major.Minor.Patch.Development (e.g. ExampleSoftware v1.1.1.1). Datasets stored on the Harvard Dataverse use the simpler Major.Minor convention.  -->
<!-- MOSHEMs developed in R can take advantage of templates provided by devtools [@devtools2021] and pkgdown [@pkgdown2022] to run continuous integration checks on GitHub.  --><!-- These tests can include those of units (do individual functions produce expected output?), documentation (does documentation render correctly?; can all example workflows be executed?) and installation (can the software be successfully deployed on multiple types of operating system?).  -->
<!-- MH systems design is not a pharma led project - less concerns about commercial ownership -->

<!-- and for mental health specifically [@Occhipinti2021],  -->
<!-- could provide greater insight about inter-dependencies between candidate policies and the dynamic nature of the mental health systems planning context. Dynamic systems methods might provide the foundations for developing   -->
<!-- and models designed for multiple purposes r -->

 <!-- as part of the Open Value Initiative [@Jansen2019].  -->
<!-- Similarly, developing partnerships between modellers and decision-makers across the life-cycle of a modelling project can help ensure models are appropriately conceptualised, implemented and have practical utility as decision aids [@Zabell2021; @SQUIRES2016588]. -->
 <!-- Yet single purpose models that assume static systems may be inadequate for the decision support needs of policymakers and service planners [@PC2020; @Occhipinti2021].  -->
<!-- The large and widespread additional mental health burdens recently observed during the COVID-19 pandemic[@20211700] and predicted as a potential future consequence of climate change [@page_howard_2010], highlight the need to improve the resilience and adaptability of these systems. To help stem growing demand for mental health services, policymakers have also been encouraged to place greater emphasis on tackling the social determinants of mental disorders [@RN11].  -->

<!-- Our approach to model development is to undertake a number of discrete modelling projects of the people, places, platforms and programs that shape the mental health and wellbeing of young people and to progressively link them together by means of a common framework. To model people we are developing synthetic representations of populations of interest [@DVN/HJXYKQ_2021] that describe relevant individual characteristics and their relationships, algorithms that map psychological measures to health utility [@Hamilton2021.07.07.21260129] and choice models for predicting the helpseeking behaviour of young people [@hamilton_matthew_2022_6627995]. Our in-development model of places [@DVN/V3OKZV_2022] has the aim of synthesising geometry and spatial attribute data to characterise the geographic distribution of relevant demographic, environmental, epidemiological and service infrastructure features. We are in the early stages of a multi-annual project to develop a service platform model that will represent the processes and operations of a complex primary youth mental health service. We also plan to extend and update our prior work reviewing economic evidence relating to youth mental health programs [@RN33] so that it can be integrated with this model. Our initial work on *readyforwhatsnext* is focused on Victoria, Australia but the framework we are using to develop it is designed to facilitate extension by ourselves and others to different jurisdictional decision contexts. Progress is reported on a project website [@rfwn2022]. -->


