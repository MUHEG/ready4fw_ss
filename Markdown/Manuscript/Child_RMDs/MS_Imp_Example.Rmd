---
title: "MS_Imp_Guidelines"
output: html_document
date: '2022-07-21'
---

<!-- Although we are principally interested in using CHEMs to answer policy questions relating to the mental health of young people in Australia, we want to facilitate CHEM transferability to other jurisdictions.  -->
Our early use of the ready4 framework has focused on developing readyforwhatsnext modules for constructing, sharing and using utility mapping models in youth mental health.

## Model user requirements
The initial model user requirements (MURs) that we specified for initial readyforwhatsnext modules we developed for utility mapping were:

MUR 1) *Variable validation tools* that: (i) define the allowable range (minimum and maximum) and numeric class (integer or double precision) associated with different psychological, functional and health utility summary measures appropriate for use in youth mental health samples; (ii) provide an informative error message that halts an analysis or reporting program's execution should impermissible values be assigned to each measure.

MUR 2) *Dataset description tools* that: (i) attach metadata to a human records dataset that specifies the unique person identifier, group assignment and data collection round variables; and (ii) generate tabular and graphical summaries of dataset descriptive statistics.

MUR 3) *Multi-attribute instrument tools* that: (i) attaches structured metadata about an instrument (e.g. name, version, country, items, domains, parameter values for scoring algorithms) to a human records dataset; (ii) applies a pre-written (for AQol-6D and EQ-5D instruments) or user-supplied instrument scoring function to a human records dataset; and (iii) generates descriptive plots of a dataset's instrument scores for individual items, domains and totals (weighted and unweighted).

MUR 4) *Utility mapping model construction tools* that: (i) attach structured metadata about the target utility instrument and candidate predictor variables and covariates to a human records dataset; (ii) generate descriptive tables, a correlation matrix and plots about the variables to be assessed for inclusion in models; (iii) attach structured metadata about the candidate utility mapping models to be assessed; (iv) generate tabular and graphical summaries of the performance of each combination of model, predictors and covariates specified for evaluation by either a user or by a default algorithm; (v) strip confidential records from the constructed models so that they are able to be publicly shared.

MUR 5) *Utility mapping reporting tools* that: (i) generate from a reporting template a PDF catalogue summarising the predictive performance under multiple configurations of utility mapping models selected for public dissemination; (ii) generate from a reporting template (either a pre-written default or a user-supplied customisation) a scientific summary manuscript summarising a utility mapping study; (iii) share model catalogues and (if desired) scientific summary along with metadata about utility mapping models in an open access repository; and (iv) facilitate the authoring of a single consolidated study reproducibility program that implements all steps inclusive of data ingest, instrument scoring, model construction, report authoring and dissemination of study artefacts.

MUR 6) *Utility prediction tools* that: (i) search open access repositories for utility mapping models developed with readyforwhastnext; (ii) retrieve relevant metadata, including a link to the model catalogue for models specified by a user; (iii) applies selected utility mapping model to make out of sample predictions using a dataset provided by a user; and (iv) (where there is health utility data at two time points) convert predicted health utility scores to Quality Adjusted Life Years (QALYs).

## Model implementation

### Online service configuration
We established and configured accounts with the online services supported by ready4. We created a GitHub organisation (a collection of code repositories) where source code that we author is stored and version controlled at https://github.com/ready4-dev/. We configured individual repositories in our GitHub organisation to implement continuous integration to assess the compliance of our module libraries with policies specified by the Comprehensive R Archive Network (CRAN) [@CRAN2022]. <!-- To track our code coverage, we linked our GitHub organisation to an account we established at codecov [@codecov_2022].  --> To facilitate the creation and hosting of module library websites, we enabled GitHub Pages in each code repository. We also created a Zenodo community, a collection of permanent, uniquely identified repositories, at https://zenodo.org/communities/ready4/. We then linked our Zenodo community and GitHub organisation so that every time we specify a version of code in one of our GitHub repositories as a “release”, a copy of that code is automatically archived on Zenodo with a DOI. Finally, to manage model datasets, we created a dedicated collection (https://dataverse.harvard.edu/dataverse/ready4) within the Harvard Dataverse installation.

<!-- To track our code coverage, we linked our GitHub organisation to an account we established at codecov [@codecov_2022].  -->
<!-- The framework also supports integrations with the open science repositories Zenodo [@Zenodo2013] and Dataverse [@Dataverse2007] that provide persistent storage solutions that generate a Digital Object Identifier (DOI) for each code and data collection. -->
<!-- The third party services that our framework is designed to work with include GitHub, Zenodo, Dataverse, -->
<!-- codecov [@codecov_2022] -->

### Outputs
We have previously described a study [@Hamilton2021.07.07.21260129] to develop utility mapping models for use in samples of young people presenting to primary mental health services. The ready4 software framework was used in that study to develop CHEM modules, supply those modules with data and implement modelling analyses, creating the following artefacts:

- development version module libraries for describing and validating youth mental health human record datasets [@hamilton_matthew_2022_6084467], scoring health utility [@hamilton_matthew_2022_6084824], specifying utility mapping models [@hamilton_matthew_2022_6116701] and implementing reproducible utility mapping studies [@gao_caroline_2022_6130155]; 

 - a development version library of functions for finding and using utility mapping models developed with these tools [@matthew_p_hamilton_2021_5646669];
 
 - collections of real data (study input and results [@DVN/DKDIB0_2021]) and toy data (synthetic populations for testing model modules [@DVN/HJXYKQ_2021]); 
 
 - programs for replicating all steps from data ingest to manuscript production [@hamilton_matthew_2022_6129906], applying utility mapping models to new data [@hamilton_matthew_2022_6416330] and generating a synthetic representation of the study dataset [@hamilton_matthew_p_2022_6321821]; and
 
 - subroutines for creating a catalogue of utility mapping models [@hamilton_matthew_2022_6116385] and generating a draft scientific manuscript [@matthew_p_hamilton_2022_5976988] for studies implemented with these modules.

```{r, child = child_docs_ls$threeb, echo=FALSE}
```

(Table \@ref(tab:rfwnlibs)).


## Documentation website
We used functions from our foundation framework library to partially automate website updates relating to available CHEM modules, datasets and analysis programs. 

## Transferability assessment
We created a checklist (Table \@ref(tab:checktb)) that we used to subjectively assess these study outputs against <!-- Edit_start -->explicit criteria for assessing transparent, reusable and updatable CHEMs that we have previously developed (add REF) <!-- Edit_end -->. For each criterion, we provided a global assessment of whether it was met using the responses “yes”, “no” or “partial”.  We believe our utility mapping study CHEMs have satisfactorily met five of the six criteria (T1, T2, R1, R2 and U1) and have partially met one criterion (U2). The main shortcomings that we identified when applying the assessment criteria was that we have yet to adequately implement unit testing of the R libraries authored as part of this study. 

\blandscape

```{r, child = child_docs_ls$four, echo=FALSE}
```

\elandscape
